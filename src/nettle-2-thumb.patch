diff --git a/arm/aes.m4 b/arm/aes.m4
index 95382de2..f452fbf7 100644
--- a/arm/aes.m4
+++ b/arm/aes.m4
@@ -33,7 +33,8 @@ define(`AES_FINAL_ROUND_V6', `
 	uxtb	T0, $3, ror #16
 	ldrb	T0, [TABLE, T0]
 	eor	$6, $6, T0, lsl #16
-	ldrb	T0, [TABLE, $4, lsr #24]
+	add		T0, TABLE, $4, lsr #24
+	ldrb	T0, [T0]
 	eor	$6, $6, T0, lsl #24
 	ldr	T0, [$5], #+4
 	eor	$6, $6, T0
@@ -51,7 +52,8 @@ define(`AES_FINAL_ROUND_V5', `
 	and	T0, $7, $3, ror #16
 	ldrb	T0, [TABLE, T0]
 	eor	$6, $6, T0, lsl #16
-	ldrb	T0, [TABLE, $4, lsr #24]
+	add		T0, TABLE, $4, lsr #24
+	ldrb	T0, [T0]
 	eor	$6, $6, T0, lsl #24
 	ldr	T0, [$5], #+4
 	eor	$6, T0
diff --git a/arm/memxor.asm b/arm/memxor.asm
index 1431a9e7..baa52649 100644
--- a/arm/memxor.asm
+++ b/arm/memxor.asm
@@ -54,7 +54,6 @@ define(`S1ADJ', IF_LE(`lsl', `lsr'))
 	.file "memxor.asm"
 
 	.text
-	.arm
 
 	C memxor(void *dst, const void *src, size_t n)
 	.align 4
@@ -107,7 +106,7 @@ PROLOGUE(nettle_memxor)
 	C With big-endian, we need to do
 	C DST[i] ^= (SRC[i] << CNT) ^ (SRC[i+1] >> TNC)
 
-	push	{r4,r5,r6}
+	push	{r4,r5,r6,r7}
 	
 	lsl	CNT, r3, #3
 	bic	SRC, #3
@@ -124,14 +123,18 @@ PROLOGUE(nettle_memxor)
 .Lmemxor_word_loop:
 	ldr	r5, [SRC], #+4
 	ldr	r3, [DST]
-	eor	r3, r3, r4, S0ADJ CNT
-	eor	r3, r3, r5, S1ADJ TNC
+	mov r7, r4, S0ADJ CNT
+	eor	r3, r7
+	mov r7, r5, S1ADJ TNC
+	eor	r3, r7
 	str	r3, [DST], #+4
 .Lmemxor_odd:
 	ldr	r4, [SRC], #+4
 	ldr	r3, [DST]
-	eor	r3, r3, r5, S0ADJ CNT
-	eor	r3, r3, r4, S1ADJ TNC
+	mov r7, r5, S0ADJ CNT
+	eor	r3, r7
+	mov r7, r4, S1ADJ TNC
+	eor	r3, r7
 	str	r3, [DST], #+4
 	subs	N, #8
 	bcs	.Lmemxor_word_loop
@@ -145,7 +148,7 @@ PROLOGUE(nettle_memxor)
 	ldr	r3, [DST]
 	eor	r3, r4
 
-	pop	{r4,r5,r6}
+	pop	{r4,r5,r6,r7}
 
 	C Store bytes, one by one.
 .Lmemxor_leftover:
@@ -160,7 +163,7 @@ IF_LE(`	lsr	r3, #8')
 	bne	.Lmemxor_leftover
 	b	.Lmemxor_bytes
 .Lmemxor_odd_done:
-	pop	{r4,r5,r6}
+	pop	{r4,r5,r6,r7}
 	bx	lr
 
 .Lmemxor_same:
diff --git a/arm/memxor3.asm b/arm/memxor3.asm
index c2b43c13..26181902 100644
--- a/arm/memxor3.asm
+++ b/arm/memxor3.asm
@@ -59,7 +59,6 @@ define(`S1ADJ', IF_LE(`lsl', `lsr'))
 	.file "memxor3.asm"
 
 	.text
-	.arm
 
 	C memxor3(void *dst, const void *a, const void *b, size_t n)
 	.align 2
@@ -67,7 +66,7 @@ PROLOGUE(nettle_memxor3)
 	cmp	N, #0
 	beq	.Lmemxor3_ret
 
-	push	{r4,r5,r6,r7,r8,r10,r11}
+	push	{r4,r5,r6,r7,r8,r9,r10,r11}
 	cmp	N, #7
 
 	add	AP, N
@@ -86,7 +85,7 @@ PROLOGUE(nettle_memxor3)
 	bne	.Lmemxor3_bytes
 
 .Lmemxor3_done:
-	pop	{r4,r5,r6,r7,r8,r10,r11}
+	pop	{r4,r5,r6,r7,r8,r9,r10,r11}
 .Lmemxor3_ret:
 	bx	lr
 
@@ -145,14 +144,18 @@ PROLOGUE(nettle_memxor3)
 .Lmemxor3_au_loop:
 	ldr	r5, [BP, #-4]!
 	ldr	r6, [AP, #-4]!
-	eor	r6, r6, r4, S1ADJ ATNC
-	eor	r6, r6, r5, S0ADJ ACNT
+	mov r9, r4, S1ADJ ATNC
+	eor	r6, r9
+	mov r9, r5, S0ADJ ACNT
+	eor	r6, r9
 	str	r6, [DST, #-4]!
 .Lmemxor3_au_odd:
 	ldr	r4, [BP, #-4]!
 	ldr	r6, [AP, #-4]!
-	eor	r6, r6, r5, S1ADJ ATNC
-	eor	r6, r6, r4, S0ADJ ACNT
+	mov r9, r5, S1ADJ ATNC
+	eor	r6, r9
+	mov r9, r4, S0ADJ ACNT
+	eor	r6, r9
 	str	r6, [DST, #-4]!
 	subs	N, #8
 	bcs	.Lmemxor3_au_loop
@@ -162,7 +165,8 @@ PROLOGUE(nettle_memxor3)
 	C Leftover bytes in r4, low end on LE and high end on BE before
 	C preparatory alignment correction
 	ldr	r5, [AP, #-4]
-	eor	r4, r5, r4, S1ADJ ATNC
+	mov r4, r4, S1ADJ ATNC
+	eor	r4, r5
 	C now byte-aligned in high end on LE and low end on BE because we're
 	C working downwards in saving the very first bytes of the buffer
 
@@ -261,14 +265,16 @@ IF_BE(`	lsr	r4, #8')
 	ldr	r6, [BP, #-4]!
 	eor	r5, r6
 	S1ADJ	r4, ATNC
-	eor	r4, r4, r5, S0ADJ ACNT
+	mov r9, r5, S0ADJ ACNT
+	eor	r4, r9
 	str	r4, [DST, #-4]!
 .Lmemxor3_uu_odd:
 	ldr	r4, [AP, #-4]!
 	ldr	r6, [BP, #-4]!
 	eor	r4, r6
 	S1ADJ	r5, ATNC
-	eor	r5, r5, r4, S0ADJ ACNT
+	mov r9, r4, S0ADJ ACNT
+	eor	r5, r9
 	str	r5, [DST, #-4]!
 	subs	N, #8
 	bcs	.Lmemxor3_uu_loop
@@ -311,17 +317,23 @@ IF_BE(`	lsr	r4, #8')
 	ldr	r5, [AP, #-4]!
 	ldr	r7, [BP, #-4]!
 	S1ADJ	r4, ATNC
-	eor	r4, r4, r6, S1ADJ BTNC
-	eor	r4, r4, r5, S0ADJ ACNT
-	eor	r4, r4, r7, S0ADJ BCNT
+	mov r9, r6, S1ADJ BTNC
+	eor	r4, r9
+	mov r9, r5, S0ADJ ACNT
+	eor	r4, r9
+	mov r9, r7, S0ADJ BCNT
+	eor	r4, r9
 	str	r4, [DST, #-4]!
 .Lmemxor3_uud_odd:
 	ldr	r4, [AP, #-4]!
 	ldr	r6, [BP, #-4]!
 	S1ADJ	r5, ATNC
-	eor	r5, r5, r7, S1ADJ BTNC
-	eor	r5, r5, r4, S0ADJ ACNT
-	eor	r5, r5, r6, S0ADJ BCNT
+	mov r9, r7, S1ADJ BTNC
+	eor	r5, r9
+	mov r9, r4, S0ADJ ACNT
+	eor	r5, r9
+	mov r9, r6, S0ADJ BCNT
+	eor	r5, r9
 	str	r5, [DST, #-4]!
 	subs	N, #8
 	bcs	.Lmemxor3_uud_loop
diff --git a/arm/v6/aes-encrypt-internal.asm b/arm/v6/aes-encrypt-internal.asm
index 6cbd66d6..c8c6b4da 100644
--- a/arm/v6/aes-encrypt-internal.asm
+++ b/arm/v6/aes-encrypt-internal.asm
@@ -30,8 +30,6 @@ ifelse(`
    not, see http://www.gnu.org/licenses/.
 ')
 
-	.arch armv6
-
 include_src(`arm/aes.m4')
 
 C	Benchmarked at at 706, 870, 963 cycles/block on cortex A9,
diff --git a/arm/v6/aes-decrypt-internal.asm b/arm/v6/aes-decrypt-internal.asm
index e8c6e91a..c8c4b40a 100644
--- a/arm/v6/aes-decrypt-internal.asm
+++ b/arm/v6/aes-decrypt-internal.asm
@@ -30,8 +30,6 @@ ifelse(`
    not, see http://www.gnu.org/licenses/.
 ')
 
-	.arch armv6
-
 include_src(`arm/aes.m4')
 
 define(`PARAM_ROUNDS', `r0')
diff --git a/arm/v6/sha1-compress.asm b/arm/v6/sha1-compress.asm
index be6170b3..9c77824d 100644
--- a/arm/v6/sha1-compress.asm
+++ b/arm/v6/sha1-compress.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "sha1-compress.asm"
-	.arch armv6
 
 define(`STATE', `r0')
 define(`INPUT', `r1')
@@ -123,9 +122,11 @@ PROLOGUE(nettle_sha1_compress)
 	ands	SHIFT, INPUT, #3
 	and	INPUT, INPUT, $-4
 	ldr	WPREV, [INPUT]
+	it ne
 	addne	INPUT, INPUT, #4	C Unaligned input
 	lsl	SHIFT, SHIFT, #3
 	mov	T0, #0
+	it ne
 	movne	T0, #-1
 IF_LE(`	lsl	W, T0, SHIFT')
 IF_BE(`	lsr	W, T0, SHIFT')
diff --git a/arm/v6/sha256-compress.asm b/arm/v6/sha256-compress.asm
index 3c021284..390965c1 100644
--- a/arm/v6/sha256-compress.asm
+++ b/arm/v6/sha256-compress.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "sha256-compress.asm"
-	.arch armv6
 
 define(`STATE', `r0')
 define(`INPUT', `r1')
@@ -133,9 +132,11 @@ PROLOGUE(_nettle_sha256_compress)
 	ands	SHIFT, INPUT, #3
 	and	INPUT, INPUT, $-4
 	ldr	I0, [INPUT]
+	it ne
 	addne	INPUT, INPUT, #4
 	lsl	SHIFT, SHIFT, #3
 	mov	T0, #0
+	it ne
 	movne	T0, #-1
 IF_LE(`	lsl	I1, T0, SHIFT')
 IF_BE(`	lsr	I1, T0, SHIFT')
diff --git a/arm/neon/chacha-3core.asm b/arm/neon/chacha-3core.asm
index c29c62a5..2a2eb648 100644
--- a/arm/neon/chacha-3core.asm
+++ b/arm/neon/chacha-3core.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "chacha-3core.asm"
-	.fpu	neon
 
 define(`DST', `r0')
 define(`SRC', `r1')
diff --git a/arm/neon/salsa20-2core.asm b/arm/neon/salsa20-2core.asm
index 4d9da79b..08703387 100644
--- a/arm/neon/salsa20-2core.asm
+++ b/arm/neon/salsa20-2core.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "salsa20-2core.asm"
-	.fpu	neon
 
 define(`DST', `r0')
 define(`SRC', `r1')
diff --git a/arm/neon/sha3-permute.asm b/arm/neon/sha3-permute.asm
index 46be4bc0..9938be9c 100644
--- a/arm/neon/sha3-permute.asm
+++ b/arm/neon/sha3-permute.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "sha3-permute.asm"
-	.fpu	neon
 
 define(`CTX', `r0')
 define(`COUNT', `r1')
diff --git a/arm/neon/sha512-compress.asm b/arm/neon/sha512-compress.asm
index 00633c16..7d26e38d 100644
--- a/arm/neon/sha512-compress.asm
+++ b/arm/neon/sha512-compress.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "sha512-compress.asm"
-	.fpu	neon
 
 define(`STATE', `r0')
 define(`INPUT', `r1')
@@ -194,6 +193,7 @@ PROLOGUE(_nettle_sha512_compress)
 	ands	SHIFT, INPUT, #7
 	and	INPUT, INPUT, #-8
 	vld1.8	{DT5}, [INPUT :64]
+	ite ne
 	addne	INPUT, INPUT, #8
 	addeq	SHIFT, SHIFT, #8
 	lsl	SHIFT, SHIFT, #3
diff --git a/arm/neon/umac-nh.asm b/arm/neon/umac-nh.asm
index 56ea6454..69651c67 100644
--- a/arm/neon/umac-nh.asm
+++ b/arm/neon/umac-nh.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "umac-nh.asm"
-	.fpu	neon
 
 define(`KEY', `r0')
 define(`LENGTH', `r1')
@@ -57,6 +56,7 @@ PROLOGUE(_nettle_umac_nh)
 	ands	SHIFT, MSG, #7
 	and	MSG, MSG, #-8
 	vld1.8	{DM}, [MSG :64]
+	ite ne
 	addne	MSG, MSG, #8
 	addeq	SHIFT, SHIFT, #8
 
diff --git a/arm/neon/umac-nh-n.asm b/arm/neon/umac-nh-n.asm
index 7e36afe2..7d6ccde5 100644
--- a/arm/neon/umac-nh-n.asm
+++ b/arm/neon/umac-nh-n.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "umac-nh.asm"
-	.fpu	neon
 
 define(`OUT', `r0')
 define(`ITERS', `r1')
@@ -69,6 +68,7 @@ PROLOGUE(_nettle_umac_nh_n)
 	ands	SHIFT, MSG, #7
 	and	MSG, MSG, #-8
 	vld1.8	{DM}, [MSG :64]
+	ite ne
 	addne	MSG, MSG, #8
 	addeq	SHIFT, SHIFT, #8
 
diff --git a/arm/ecc-secp192r1-modp.asm b/arm/ecc-secp192r1-modp.asm
index e500bc85..7a92d1fe 100644
--- a/arm/ecc-secp192r1-modp.asm
+++ b/arm/ecc-secp192r1-modp.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "ecc-secp192r1-modp.asm"
-	.arm
 
 define(`HP', `r0') C Overlaps unused modulo argument
 define(`RP', `r1')
diff --git a/arm/ecc-secp224r1-modp.asm b/arm/ecc-secp224r1-modp.asm
index 4b3b24e5..04839e06 100644
--- a/arm/ecc-secp224r1-modp.asm
+++ b/arm/ecc-secp224r1-modp.asm
@@ -31,7 +31,6 @@ ifelse(`
 ')
 
 	.file "ecc-secp224r1-modp.asm"
-	.arm
 
 define(`RP', `r1')	C Overlaps T0
 define(`XP', `r2')
@@ -44,6 +43,7 @@ define(`T3', `r5')
 define(`T4', `r6')
 define(`T5', `r7')
 define(`T6', `r8')
+define(`SC', `r9')
 define(`N3', `r10')
 define(`L0', `r11')
 define(`L1', `r12')
@@ -55,7 +55,7 @@ define(`L2', `lr')
 
 PROLOGUE(_nettle_ecc_secp224r1_modp)
 	C Pushes RP last
-	push	{r1,r4,r5,r6,r7,r8,r10,r11,lr}
+	push	{r1,r4,r5,r6,r7,r8,r9,r10,r11,lr}
 
 	add	L2, XP, #28
 	ldm	L2, {T0,T1,T2,T3,T4,T5,T6}
@@ -77,9 +77,11 @@ PROLOGUE(_nettle_ecc_secp224r1_modp)
 	sbcs	T5, T5, T2
 	sbcs	T6, T6, H
 	mov	H, #1		C This is the B^7
-	sbc	H, #0
+	mov SC, #0
+	sbc	H, SC
 	subs	T6, T6, T3
-	sbc	H, #0
+	mov SC, #0
+	sbc	H, SC
 
 	C Now subtract from low half
 	ldm	XP!, {L0,L1,L2}
@@ -95,7 +97,8 @@ PROLOGUE(_nettle_ecc_secp224r1_modp)
 	sbcs	T4, L0, T4
 	sbcs	T5, L1, T5
 	sbcs	T6, L2, T6
-	rsc	H, H, #0
+	mov SC, #0
+	rsc	H, H, SC
 
 	C Now -2 <= H <= 0 is the borrow, so subtract (B^3 - 1) |H|
 	C Use (B^3 - 1) H = <H, H, H> if -1 <=H <= 0, and
@@ -124,5 +127,5 @@ PROLOGUE(_nettle_ecc_secp224r1_modp)
 
 	stm	XP, {T0,T1,T2,T3,T4,T5,T6}
 
-	pop	{r4,r5,r6,r7,r8,r10,r11,pc}
+	pop	{r4,r5,r6,r7,r8,r9,r10,r11,pc}
 EPILOGUE(_nettle_ecc_secp224r1_modp)
